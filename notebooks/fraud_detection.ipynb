{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Card Fraud Detection\n",
    "## Final Project for the Codigo Facilito's Machine Learning 2023 Bootcamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Definition and Objectives\n",
    "\n",
    "### Problem Definition\n",
    "\n",
    "Nowadays it is very easy for malicious actors to illegaly obtain banking accounts' authentication information that allows them to access unsuspected victim's financial assets without their knowledge, until it's too late. To minimize the impact of this, different techniques can be applied to detect when a user's identity has been comprompised and their assets are being accessed illegaly.\n",
    "\n",
    "One of them are fraud detection systems, which are able to learn users' banking transactions' behaviour - which means learning the usual amounts, and when (at what time of day), where (the type of stores they commonly visit), and how (purchasing online vs swiping a physical card) they usually perform transactions with their credit cards - in order to detect when a new transaction doen't follow the previously learned patterns, flagging such transactions as fraudulent, and require the user to perform additional verification for the transaction to go through.\n",
    "\n",
    "### Problem Relevance\n",
    "\n",
    "\n",
    "Just to highlight the importance of fraud detection systems, according to the [Security.org 2023 Credit Card Fraud Report](https://www.security.org/digital-safety/credit-card-fraud-report/):\n",
    "- 65% of credit and credit card holders have been fraud victims at some point in their lives, up from 58 percent in 2022. This equates to about 151 million fraud victims in the United States alone.\n",
    "- An increasing number of Americans have been victimized multiple times: in 2022, 44 percent of credit card users reported having two or more fraudulent charges, compared to 35 percent in 2021.\n",
    "- Since 2021, the median fraudulent charge has climbed by about 27 percent (rising to $79 in 2023). This equates to about $12 billion in total attempted fraudulent charges.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Stakeholders\n",
    "\n",
    "The main stakeholders in this project are:\n",
    "\n",
    "1) The banking institution(s) that would use their clients' banking transaction data required to train the machine learning model.\n",
    "2) The banks' clients allowing for their transactions' data to be used to train the model.\n",
    "3) The FTC (in the US) and other regulatory institutions that would need to verify and approve the use of the banks' clients' data to train the model, and also approve the use of such model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objectives\n",
    "\n",
    "The goal of this project is to build a machine learning model that allows the detection of fraudulent credit card transactions by training it with a credit card transaction dataset, and build a feature engineering and training pipeline that will allow the model to be re-trained in the future.\n",
    "\n",
    "- The final machine learning model should provide at least 10% better recall than a baseline model to be defined later in the project.\n",
    "- A feature egineering and training pipeline should be used to allow future training of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation Steps\n",
    "\n",
    "1) Decide if using a machine learning model is the right solution for this problem. In this case, because we want to be more proactive when fraudulent transactions happen and not needing to wait for the user to report them, we believe using a machine learning model is an appropiate solution for this problem.\n",
    "2) Identify a public credit card transaction dataset suitable for an Exploratory Data Analysis, that allows the clear and easy identification of each column's information. Some datasets available in Kaggle contain columns that were already scaled or processed using PCA analysis, and therefore are not useful for this project's goals.\n",
    "3) Research the different machine learning models that are best suited for detecting fraudulent credit card transactions.\n",
    "4) Research different methods to improve the dataset if it is unbalanced in terms of the number of fraudulent transactions vs legitimate ones.\n",
    "5) Research different feature engineering techniques that would allow us to reduce the size of the datasets for our model.\n",
    "6) Evaluate the different deployment deployment frequencies and strategies available, and design and build the model's feature engineering and training pipeline model with them in mind."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "- We will use the [Credit Card Transactions Kaggle Dataset](https://www.kaggle.com/datasets/ealtman2019/credit-card-transactions), because it contains a good amount of data to train our model - approximately 24M transactions of 2000 users generated by a multi-agent virtual world simulation performed by IBM - and its columns are easy to identify and work with because they are not scaled or obfuscated in any way that could result in us not being able to find correlations in the data.\n",
    "\n",
    "- The dataset contains the following columns:\n",
    "    1) 'User': An ID of the user.\n",
    "    2) 'Card': An ID for the user's card, some users have multiple cards.\n",
    "    3) 'Year', 'Month', 'Day', 'Time': The timestamp of the transaction. \n",
    "    4) 'Amount': The amount of the transaction.\n",
    "    5) 'Use Chip': 'Swipe Transaction' if a physical card was used to perform the transaction, or 'Online Transaction' if the transaction was performed online.\n",
    "    6) 'Merchant Name': The ID of the store where the transaction was made.\n",
    "    7) 'Merchant City', 'Merchant State', 'Zip': The store's location.\n",
    "    8) 'MCC': The [Merchant Category Code](https://www.investopedia.com/terms/m/merchant-category-codes-mcc.asp).\n",
    "    9) 'Errors?': Any error(s) during the transaction, eg. 'Insufficient Balance', 'Technical Glitch', etc.\n",
    "    10) 'Is Fraud?: A label indicating if the transaction was fraudulent or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deployment Plan\n",
    "\n",
    "#### Deployment Frequency\n",
    "\n",
    "Because we don't need to process the bank transactions and tell if they are legitimate or fraudulent as soon as they happen, we can use an Streaming Deployment for this solution. This will allow us to have more flexibility when choosing the right model for this problem, and also the response times are better than with other Deployment approaches.\n",
    "\n",
    "One of the other two options, making a Batch Deployment of the model, would not be fit for this purpose since taking groups of transactions and making predictions about them later would probably take longer than in a streaming deployment, and would probably make for a poor user experience, so using Batch Deployment for this problem would fall short for our needs.\n",
    "\n",
    "And the remaining option, using Online Deployment, may work for this purpose but it would be an overkill for this scenario, since we really don't need to decide if a transaction is fraudulent or not immediately after it has been made - the final decision could be done even later, after a more through investigation, so using an Online Deployment for this solution would not be a good use of our resources to achieve it.\n",
    "\n",
    "#### Deployment Strategy\n",
    "\n",
    "After considering the different deployment strategies available, using a Rolling/Ramped Update strategy should be optimal for this case, because it would allow the users to keep making transactions without downtime, and for this case in particular it's not crucial to guarantee the user is using a particular version of our model (the new one \"B\" vs the old one \"A\"), as long as the users can keep performing transactions and them getting evaluated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "We will begin by loading the credit card transaction dataset into a polars DataFrame and confirm the contents of the file have been loaded successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (24_386_900, 15)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>User</th><th>Card</th><th>Year</th><th>Month</th><th>Day</th><th>Time</th><th>Amount</th><th>Use Chip</th><th>Merchant Name</th><th>Merchant City</th><th>Merchant State</th><th>Zip</th><th>MCC</th><th>Errors?</th><th>Is Fraud?</th></tr><tr><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>i64</td><td>str</td><td>str</td><td>str</td><td>i64</td><td>str</td><td>str</td><td>f64</td><td>i64</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>0</td><td>0</td><td>2002</td><td>9</td><td>1</td><td>&quot;06:21&quot;</td><td>&quot;$134.09&quot;</td><td>&quot;Swipe Transact…</td><td>3527213246127876953</td><td>&quot;La Verne&quot;</td><td>&quot;CA&quot;</td><td>91750.0</td><td>5300</td><td>null</td><td>&quot;No&quot;</td></tr><tr><td>0</td><td>0</td><td>2002</td><td>9</td><td>1</td><td>&quot;06:42&quot;</td><td>&quot;$38.48&quot;</td><td>&quot;Swipe Transact…</td><td>-727612092139916043</td><td>&quot;Monterey Park&quot;</td><td>&quot;CA&quot;</td><td>91754.0</td><td>5411</td><td>null</td><td>&quot;No&quot;</td></tr><tr><td>0</td><td>0</td><td>2002</td><td>9</td><td>2</td><td>&quot;06:22&quot;</td><td>&quot;$120.34&quot;</td><td>&quot;Swipe Transact…</td><td>-727612092139916043</td><td>&quot;Monterey Park&quot;</td><td>&quot;CA&quot;</td><td>91754.0</td><td>5411</td><td>null</td><td>&quot;No&quot;</td></tr><tr><td>0</td><td>0</td><td>2002</td><td>9</td><td>2</td><td>&quot;17:45&quot;</td><td>&quot;$128.95&quot;</td><td>&quot;Swipe Transact…</td><td>3414527459579106770</td><td>&quot;Monterey Park&quot;</td><td>&quot;CA&quot;</td><td>91754.0</td><td>5651</td><td>null</td><td>&quot;No&quot;</td></tr><tr><td>0</td><td>0</td><td>2002</td><td>9</td><td>3</td><td>&quot;06:23&quot;</td><td>&quot;$104.71&quot;</td><td>&quot;Swipe Transact…</td><td>5817218446178736267</td><td>&quot;La Verne&quot;</td><td>&quot;CA&quot;</td><td>91750.0</td><td>5912</td><td>null</td><td>&quot;No&quot;</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>1999</td><td>1</td><td>2020</td><td>2</td><td>27</td><td>&quot;22:23&quot;</td><td>&quot;$-54.00&quot;</td><td>&quot;Chip Transacti…</td><td>-5162038175624867091</td><td>&quot;Merrimack&quot;</td><td>&quot;NH&quot;</td><td>3054.0</td><td>5541</td><td>null</td><td>&quot;No&quot;</td></tr><tr><td>1999</td><td>1</td><td>2020</td><td>2</td><td>27</td><td>&quot;22:24&quot;</td><td>&quot;$54.00&quot;</td><td>&quot;Chip Transacti…</td><td>-5162038175624867091</td><td>&quot;Merrimack&quot;</td><td>&quot;NH&quot;</td><td>3054.0</td><td>5541</td><td>null</td><td>&quot;No&quot;</td></tr><tr><td>1999</td><td>1</td><td>2020</td><td>2</td><td>28</td><td>&quot;07:43&quot;</td><td>&quot;$59.15&quot;</td><td>&quot;Chip Transacti…</td><td>2500998799892805156</td><td>&quot;Merrimack&quot;</td><td>&quot;NH&quot;</td><td>3054.0</td><td>4121</td><td>null</td><td>&quot;No&quot;</td></tr><tr><td>1999</td><td>1</td><td>2020</td><td>2</td><td>28</td><td>&quot;20:10&quot;</td><td>&quot;$43.12&quot;</td><td>&quot;Chip Transacti…</td><td>2500998799892805156</td><td>&quot;Merrimack&quot;</td><td>&quot;NH&quot;</td><td>3054.0</td><td>4121</td><td>null</td><td>&quot;No&quot;</td></tr><tr><td>1999</td><td>1</td><td>2020</td><td>2</td><td>28</td><td>&quot;23:10&quot;</td><td>&quot;$45.13&quot;</td><td>&quot;Chip Transacti…</td><td>4751695835751691036</td><td>&quot;Merrimack&quot;</td><td>&quot;NH&quot;</td><td>3054.0</td><td>5814</td><td>null</td><td>&quot;No&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (24_386_900, 15)\n",
       "┌──────┬──────┬──────┬───────┬───┬─────────┬──────┬─────────┬───────────┐\n",
       "│ User ┆ Card ┆ Year ┆ Month ┆ … ┆ Zip     ┆ MCC  ┆ Errors? ┆ Is Fraud? │\n",
       "│ ---  ┆ ---  ┆ ---  ┆ ---   ┆   ┆ ---     ┆ ---  ┆ ---     ┆ ---       │\n",
       "│ i64  ┆ i64  ┆ i64  ┆ i64   ┆   ┆ f64     ┆ i64  ┆ str     ┆ str       │\n",
       "╞══════╪══════╪══════╪═══════╪═══╪═════════╪══════╪═════════╪═══════════╡\n",
       "│ 0    ┆ 0    ┆ 2002 ┆ 9     ┆ … ┆ 91750.0 ┆ 5300 ┆ null    ┆ No        │\n",
       "│ 0    ┆ 0    ┆ 2002 ┆ 9     ┆ … ┆ 91754.0 ┆ 5411 ┆ null    ┆ No        │\n",
       "│ 0    ┆ 0    ┆ 2002 ┆ 9     ┆ … ┆ 91754.0 ┆ 5411 ┆ null    ┆ No        │\n",
       "│ 0    ┆ 0    ┆ 2002 ┆ 9     ┆ … ┆ 91754.0 ┆ 5651 ┆ null    ┆ No        │\n",
       "│ 0    ┆ 0    ┆ 2002 ┆ 9     ┆ … ┆ 91750.0 ┆ 5912 ┆ null    ┆ No        │\n",
       "│ …    ┆ …    ┆ …    ┆ …     ┆ … ┆ …       ┆ …    ┆ …       ┆ …         │\n",
       "│ 1999 ┆ 1    ┆ 2020 ┆ 2     ┆ … ┆ 3054.0  ┆ 5541 ┆ null    ┆ No        │\n",
       "│ 1999 ┆ 1    ┆ 2020 ┆ 2     ┆ … ┆ 3054.0  ┆ 5541 ┆ null    ┆ No        │\n",
       "│ 1999 ┆ 1    ┆ 2020 ┆ 2     ┆ … ┆ 3054.0  ┆ 4121 ┆ null    ┆ No        │\n",
       "│ 1999 ┆ 1    ┆ 2020 ┆ 2     ┆ … ┆ 3054.0  ┆ 4121 ┆ null    ┆ No        │\n",
       "│ 1999 ┆ 1    ┆ 2020 ┆ 2     ┆ … ┆ 3054.0  ┆ 5814 ┆ null    ┆ No        │\n",
       "└──────┴──────┴──────┴───────┴───┴─────────┴──────┴─────────┴───────────┘"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import polars as pl  # noqa: D100\n",
    "from polars import DataFrame\n",
    "\n",
    "pl.Config(tbl_rows=10)\n",
    "\n",
    "def load_data(filename: str) -> DataFrame:\n",
    "    \"\"\" Read CSV file with our dataset. \"\"\"\n",
    "    data_df = pl.read_csv(filename)\n",
    "    return data_df\n",
    "\n",
    "data_df = load_data(\"../data/credit_card_transactions-ibm_v2.csv\")\n",
    "data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we can identify a few columns that need some work, for that we will create a function called preprocess_dataset that will do the following:\n",
    "- Transform the Time column into \"Hour\" and \"Minute\" columns, instead of a string.\n",
    "- Convert the Amount column into an actual number instead of a string.\n",
    "- Make the Merchant Name a string, and then make it categorical.\n",
    "- Fill the nulls with \"ONLINE\" in the Merchant State column.\n",
    "- Convert the Zip to a string and replace the nulls with \"ONLINE\".\n",
    "- Fill the nulls in Errors? with \"No\".\n",
    "- Convert the IsFraud? column to binary format: 1 if it's fraud, and 0 if legitimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(data_df: DataFrame) -> DataFrame:\n",
    "    \"\"\" Preprocess dataset for EDA. \"\"\"\n",
    "    new_data_df = data_df.with_columns(\n",
    "        Hour = pl.col(\"Time\").map_elements(\n",
    "            lambda x: x[:2]).cast(pl.Int64, strict=True),\n",
    "        Minute = pl.col(\"Time\").map_elements(\n",
    "            lambda x: x[3:]).cast(pl.Int64, strict=True),\n",
    "    ).drop(\"Time\").with_columns(\n",
    "        pl.col(\"Amount\").map_elements(\n",
    "            lambda x: x.replace(\"$\", \"\")).cast(pl.Float64, strict=True)\n",
    "    ).with_columns(\n",
    "        pl.col(\"Merchant State\").fill_null(\"ONLINE\")\n",
    "    ).with_columns(\n",
    "        pl.col(\"Zip\").cast(pl.String, strict=True).fill_null(\"ONLINE\")\n",
    "    ).with_columns(\n",
    "        pl.col(\"Errors?\").fill_null(value=\"No\")\n",
    "    ).with_columns(\n",
    "        pl.col(\"Is Fraud?\").map_elements(\n",
    "            lambda x: 0 if x == \"No\" else 1\n",
    "        )\n",
    "    )\n",
    "    return new_data_df\n",
    "new_data_df = preprocess_dataset(data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see how the Amount varies between legitimate and fraudulent transactions. For that we will use altair to chart the top 100 amounts used for transactions, with a function called top_100_amount_counts_chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  #altair-viz-52e8d82bfeec48c2964198b8391e6728.vega-embed {\n",
       "    width: 100%;\n",
       "    display: flex;\n",
       "  }\n",
       "\n",
       "  #altair-viz-52e8d82bfeec48c2964198b8391e6728.vega-embed details,\n",
       "  #altair-viz-52e8d82bfeec48c2964198b8391e6728.vega-embed details summary {\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div id=\"altair-viz-52e8d82bfeec48c2964198b8391e6728\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-52e8d82bfeec48c2964198b8391e6728\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-52e8d82bfeec48c2964198b8391e6728\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.16.3?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"5.16.3\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-a9c59543b52347b656a7134e304748a6\"}, \"mark\": {\"type\": \"bar\"}, \"encoding\": {\"color\": {\"condition\": {\"test\": \"(datum.Count === 250980.0)\", \"value\": \"orange\"}, \"value\": \"steelblue\"}, \"x\": {\"axis\": {\"labelAngle\": -45}, \"field\": \"Amount\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"Count\", \"type\": \"quantitative\"}}, \"title\": \"Top 100 Legitimate Amounts\", \"width\": 400, \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.16.3.json\", \"datasets\": {\"data-a9c59543b52347b656a7134e304748a6\": [{\"Amount\": 7.15, \"Count\": 250980}, {\"Amount\": 0.71, \"Count\": 242192}, {\"Amount\": 16.54, \"Count\": 193618}, {\"Amount\": 7.1, \"Count\": 152877}, {\"Amount\": 22.75, \"Count\": 106528}, {\"Amount\": 6.71, \"Count\": 86934}, {\"Amount\": 125.77, \"Count\": 63106}, {\"Amount\": 0.25, \"Count\": 41909}, {\"Amount\": 15.06, \"Count\": 23031}, {\"Amount\": 43.21, \"Count\": 22894}, {\"Amount\": 14.28, \"Count\": 22887}, {\"Amount\": 9.55, \"Count\": 22792}, {\"Amount\": 8.61, \"Count\": 22782}, {\"Amount\": 21.16, \"Count\": 22757}, {\"Amount\": 1.35, \"Count\": 22718}, {\"Amount\": 4.07, \"Count\": 22672}, {\"Amount\": 12.37, \"Count\": 22654}, {\"Amount\": 2.44, \"Count\": 22653}, {\"Amount\": 56.44, \"Count\": 22596}, {\"Amount\": 10.78, \"Count\": 22577}, {\"Amount\": 8.43, \"Count\": 22565}, {\"Amount\": 13.06, \"Count\": 22536}, {\"Amount\": 25.12, \"Count\": 22448}, {\"Amount\": 27.31, \"Count\": 22399}, {\"Amount\": 7.59, \"Count\": 22377}, {\"Amount\": 8.09, \"Count\": 22364}, {\"Amount\": 59.37, \"Count\": 22355}, {\"Amount\": 36.11, \"Count\": 22346}, {\"Amount\": 41.04, \"Count\": 22331}, {\"Amount\": 35.32, \"Count\": 22312}, {\"Amount\": 11.19, \"Count\": 22274}, {\"Amount\": 1.4, \"Count\": 22267}, {\"Amount\": 1.19, \"Count\": 22248}, {\"Amount\": 32.9, \"Count\": 22242}, {\"Amount\": 17.06, \"Count\": 22200}, {\"Amount\": 8.3, \"Count\": 22193}, {\"Amount\": 26.84, \"Count\": 22127}, {\"Amount\": 18.56, \"Count\": 22089}, {\"Amount\": 1.9, \"Count\": 22077}, {\"Amount\": 3.35, \"Count\": 22045}, {\"Amount\": 7.64, \"Count\": 22031}, {\"Amount\": 21.81, \"Count\": 22005}, {\"Amount\": 18.4, \"Count\": 22000}, {\"Amount\": 6.97, \"Count\": 22000}, {\"Amount\": 27.49, \"Count\": 21996}, {\"Amount\": 0.33, \"Count\": 21959}, {\"Amount\": 22.68, \"Count\": 21957}, {\"Amount\": 36.2, \"Count\": 21914}, {\"Amount\": 8.52, \"Count\": 21911}, {\"Amount\": 14.79, \"Count\": 21872}, {\"Amount\": 11.82, \"Count\": 21849}, {\"Amount\": 11.76, \"Count\": 21830}, {\"Amount\": 1.92, \"Count\": 21815}, {\"Amount\": 8.6, \"Count\": 21814}, {\"Amount\": 0.72, \"Count\": 21801}, {\"Amount\": 15.07, \"Count\": 21773}, {\"Amount\": 13.16, \"Count\": 21766}, {\"Amount\": 16.71, \"Count\": 21726}, {\"Amount\": 41.07, \"Count\": 21721}, {\"Amount\": 1.18, \"Count\": 21706}, {\"Amount\": 1.6, \"Count\": 21645}, {\"Amount\": 17.04, \"Count\": 21631}, {\"Amount\": 18.54, \"Count\": 21622}, {\"Amount\": 1.89, \"Count\": 21614}, {\"Amount\": 34.56, \"Count\": 21608}, {\"Amount\": 59.25, \"Count\": 21608}, {\"Amount\": 59.39, \"Count\": 21597}, {\"Amount\": 13.07, \"Count\": 21597}, {\"Amount\": 21.17, \"Count\": 21581}, {\"Amount\": 27.32, \"Count\": 21578}, {\"Amount\": 0.32, \"Count\": 21574}, {\"Amount\": 14.3, \"Count\": 21569}, {\"Amount\": 14.65, \"Count\": 21561}, {\"Amount\": 7.14, \"Count\": 21536}, {\"Amount\": 22.69, \"Count\": 21519}, {\"Amount\": 36.13, \"Count\": 21500}, {\"Amount\": 7.58, \"Count\": 21495}, {\"Amount\": 25.11, \"Count\": 21480}, {\"Amount\": 43.19, \"Count\": 21471}, {\"Amount\": 20.81, \"Count\": 21465}, {\"Amount\": 8.1, \"Count\": 21459}, {\"Amount\": 5.06, \"Count\": 21438}, {\"Amount\": 3.36, \"Count\": 21426}, {\"Amount\": 1.34, \"Count\": 21423}, {\"Amount\": 21.79, \"Count\": 21422}, {\"Amount\": 8.28, \"Count\": 21421}, {\"Amount\": 10.77, \"Count\": 21412}, {\"Amount\": 8.53, \"Count\": 21407}, {\"Amount\": 9.56, \"Count\": 21396}, {\"Amount\": 26.85, \"Count\": 21390}, {\"Amount\": 1.91, \"Count\": 21380}, {\"Amount\": 16.53, \"Count\": 21376}, {\"Amount\": 14.77, \"Count\": 21370}, {\"Amount\": 1.42, \"Count\": 21364}, {\"Amount\": 7.65, \"Count\": 21357}, {\"Amount\": 6.98, \"Count\": 21337}, {\"Amount\": 11.75, \"Count\": 21333}, {\"Amount\": 56.45, \"Count\": 21330}, {\"Amount\": 38.45, \"Count\": 21320}, {\"Amount\": 18.41, \"Count\": 21266}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import altair as alt  # noqa: E402\n",
    "from altair import Chart  # noqa: E402\n",
    "from polars import DataFrame  # noqa: E402\n",
    "\n",
    "\n",
    "def top_100_amount_counts_chart(data_df: DataFrame, source: str) -> Chart:\n",
    "    \"\"\" Calculate and chart the top 100 Amounts used in transactions. \"\"\"\n",
    "    data_df = data_df.top_k(100, by=\"Count\")\n",
    "    top_amount = data_df.top_k(1, by=\"Count\").to_numpy()[0][1]\n",
    "    return alt.Chart(\n",
    "        data_df,\n",
    "        title=f\"Top 100 {source} Amounts\").mark_bar().encode(\n",
    "        x=alt.X('Amount', axis=alt.Axis(labelAngle=-45)),\n",
    "        y=\"Count\",\n",
    "        color=alt.condition(\n",
    "            alt.datum.Count == top_amount,\n",
    "            alt.value('orange'),\n",
    "            alt.value('steelblue'))\n",
    "    ).properties(width=400)\n",
    "\n",
    "legit = new_data_df.select(\n",
    "    pl.col(\"Amount\"), pl.col(\"Is Fraud?\")).filter(pl.col(\"Is Fraud?\") == 0\n",
    "    ).select(\n",
    "        pl.col(\"Amount\").unique(), pl.col(\"Amount\").unique_counts().alias(\"Count\")\n",
    "    )\n",
    "\n",
    "top_100_amount_counts_chart(legit, \"Legitimate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  #altair-viz-5693b266777143e5bbb01feed0930fd6.vega-embed {\n",
       "    width: 100%;\n",
       "    display: flex;\n",
       "  }\n",
       "\n",
       "  #altair-viz-5693b266777143e5bbb01feed0930fd6.vega-embed details,\n",
       "  #altair-viz-5693b266777143e5bbb01feed0930fd6.vega-embed details summary {\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div id=\"altair-viz-5693b266777143e5bbb01feed0930fd6\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-5693b266777143e5bbb01feed0930fd6\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-5693b266777143e5bbb01feed0930fd6\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.16.3?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"5.16.3\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-5d08550679e4f264610ac92789b59c0c\"}, \"mark\": {\"type\": \"bar\"}, \"encoding\": {\"color\": {\"condition\": {\"test\": \"(datum.Count === 25.0)\", \"value\": \"orange\"}, \"value\": \"steelblue\"}, \"x\": {\"axis\": {\"labelAngle\": -45}, \"field\": \"Amount\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"Count\", \"type\": \"quantitative\"}}, \"title\": \"Top 100 Fraudulent Amounts\", \"width\": 400, \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.16.3.json\", \"datasets\": {\"data-5d08550679e4f264610ac92789b59c0c\": [{\"Amount\": 5.6, \"Count\": 25}, {\"Amount\": 2.94, \"Count\": 22}, {\"Amount\": 32.93, \"Count\": 20}, {\"Amount\": 1.31, \"Count\": 20}, {\"Amount\": 16.84, \"Count\": 17}, {\"Amount\": 27.11, \"Count\": 17}, {\"Amount\": 33.46, \"Count\": 16}, {\"Amount\": 1.42, \"Count\": 16}, {\"Amount\": 2.07, \"Count\": 16}, {\"Amount\": 42.5, \"Count\": 16}, {\"Amount\": 4.83, \"Count\": 15}, {\"Amount\": 24.16, \"Count\": 15}, {\"Amount\": 27.33, \"Count\": 15}, {\"Amount\": 8.96, \"Count\": 15}, {\"Amount\": 10.0, \"Count\": 14}, {\"Amount\": 23.0, \"Count\": 14}, {\"Amount\": 27.42, \"Count\": 14}, {\"Amount\": 11.45, \"Count\": 14}, {\"Amount\": 8.09, \"Count\": 13}, {\"Amount\": 18.06, \"Count\": 13}, {\"Amount\": 17.39, \"Count\": 13}, {\"Amount\": 39.78, \"Count\": 13}, {\"Amount\": 10.34, \"Count\": 13}, {\"Amount\": 28.08, \"Count\": 13}, {\"Amount\": 2.28, \"Count\": 13}, {\"Amount\": 10.18, \"Count\": 13}, {\"Amount\": 45.78, \"Count\": 13}, {\"Amount\": 14.36, \"Count\": 13}, {\"Amount\": 33.32, \"Count\": 13}, {\"Amount\": 26.61, \"Count\": 12}, {\"Amount\": 15.55, \"Count\": 12}, {\"Amount\": 62.22, \"Count\": 12}, {\"Amount\": 27.7, \"Count\": 12}, {\"Amount\": 10.1, \"Count\": 12}, {\"Amount\": 3.31, \"Count\": 12}, {\"Amount\": 19.12, \"Count\": 11}, {\"Amount\": 5.5, \"Count\": 11}, {\"Amount\": 27.24, \"Count\": 11}, {\"Amount\": 3.81, \"Count\": 11}, {\"Amount\": 54.74, \"Count\": 11}, {\"Amount\": 5.47, \"Count\": 11}, {\"Amount\": 11.22, \"Count\": 11}, {\"Amount\": 1.19, \"Count\": 11}, {\"Amount\": 18.88, \"Count\": 11}, {\"Amount\": 3.19, \"Count\": 11}, {\"Amount\": 8.56, \"Count\": 11}, {\"Amount\": 12.68, \"Count\": 11}, {\"Amount\": 5.87, \"Count\": 11}, {\"Amount\": 3.67, \"Count\": 11}, {\"Amount\": 5.56, \"Count\": 11}, {\"Amount\": 13.1, \"Count\": 11}, {\"Amount\": 2.83, \"Count\": 11}, {\"Amount\": 6.4, \"Count\": 10}, {\"Amount\": 10.06, \"Count\": 10}, {\"Amount\": 9.7, \"Count\": 10}, {\"Amount\": 2.56, \"Count\": 10}, {\"Amount\": 62.13, \"Count\": 10}, {\"Amount\": 61.77, \"Count\": 10}, {\"Amount\": 7.3, \"Count\": 10}, {\"Amount\": 11.03, \"Count\": 10}, {\"Amount\": 27.59, \"Count\": 10}, {\"Amount\": 85.3, \"Count\": 10}, {\"Amount\": 7.73, \"Count\": 10}, {\"Amount\": 28.42, \"Count\": 10}, {\"Amount\": 60.23, \"Count\": 10}, {\"Amount\": 5.75, \"Count\": 10}, {\"Amount\": 18.17, \"Count\": 10}, {\"Amount\": 2.24, \"Count\": 10}, {\"Amount\": 49.91, \"Count\": 10}, {\"Amount\": 10.29, \"Count\": 10}, {\"Amount\": 1.78, \"Count\": 9}, {\"Amount\": 64.62, \"Count\": 9}, {\"Amount\": 22.87, \"Count\": 9}, {\"Amount\": 16.47, \"Count\": 9}, {\"Amount\": 57.03, \"Count\": 9}, {\"Amount\": 55.53, \"Count\": 9}, {\"Amount\": 30.21, \"Count\": 9}, {\"Amount\": 36.11, \"Count\": 9}, {\"Amount\": 66.05, \"Count\": 9}, {\"Amount\": 40.36, \"Count\": 9}, {\"Amount\": 26.64, \"Count\": 9}, {\"Amount\": 9.17, \"Count\": 9}, {\"Amount\": 8.59, \"Count\": 9}, {\"Amount\": 47.14, \"Count\": 9}, {\"Amount\": 63.5, \"Count\": 9}, {\"Amount\": 8.68, \"Count\": 9}, {\"Amount\": 77.6, \"Count\": 9}, {\"Amount\": 1.56, \"Count\": 9}, {\"Amount\": 9.54, \"Count\": 9}, {\"Amount\": 15.42, \"Count\": 9}, {\"Amount\": 18.54, \"Count\": 9}, {\"Amount\": 5.63, \"Count\": 9}, {\"Amount\": 12.6, \"Count\": 9}, {\"Amount\": 30.68, \"Count\": 9}, {\"Amount\": 1.68, \"Count\": 9}, {\"Amount\": 18.7, \"Count\": 9}, {\"Amount\": 5.17, \"Count\": 9}, {\"Amount\": 6.01, \"Count\": 8}, {\"Amount\": 66.19, \"Count\": 8}, {\"Amount\": 12.37, \"Count\": 8}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fraud = new_data_df.select(\n",
    "    pl.col(\"Amount\"), pl.col(\"Is Fraud?\")).filter(pl.col(\"Is Fraud?\") == 1\n",
    "    ).select(\n",
    "        pl.col(\"Amount\").unique(), pl.col(\"Amount\").unique_counts().alias(\"Count\")\n",
    "    )\n",
    "top_100_amount_counts_chart(fraud, \"Fraudulent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the amounts in fraudulent transactions are rarely repeated in comparison with the legitimate ones.\n",
    "\n",
    "Now let's see how the Use Chip column varies between legitimate and fraudulent transactions with the following use_chip_chart function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def use_chip_chart(data_df: DataFrame, source: str) -> Chart:\n",
    "    \"\"\" Chart the proportion of Use chip in transactions. \"\"\"\n",
    "    return alt.Chart(\n",
    "        data_df,\n",
    "        title=f\"{source} Use Chip\").mark_arc(innerRadius=70).encode(\n",
    "        color=alt.Color(\"Use Chip\", title=\"Use Chip\", type=\"nominal\",\n",
    "                    sort='ascending',\n",
    "                    scale=alt.Scale(scheme='plasma')),\n",
    "        theta=\"Count\"\n",
    "    ).properties(width=400)\n",
    "\n",
    "legit_chip = new_data_df.select(\n",
    "    pl.col(\"Use Chip\"), pl.col(\"Is Fraud?\")).filter(pl.col(\"Is Fraud?\") == 0\n",
    "    ).group_by(\"Use Chip\").agg(\n",
    "        pl.col(\"Is Fraud?\").count().alias(\"Count\")\n",
    "    )\n",
    "fraud_chip = new_data_df.select(\n",
    "    pl.col(\"Use Chip\"), pl.col(\"Is Fraud?\")).filter(pl.col(\"Is Fraud?\") == 1\n",
    "    ).group_by(\"Use Chip\").agg(\n",
    "        pl.col(\"Is Fraud?\").count().alias(\"Count\")\n",
    "    )\n",
    "use_chip_chart(legit_chip, \"Legitimate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_chip_chart(fraud_chip, \"Fraudulent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, cards are used online more frequently for fraudulent transactions than for legitimate ones, where swipe transactions are more common.\n",
    "\n",
    "Now let's see if the locations vary between legitimate and fraudulent transactions with the following top_10_merchant_state_chart and top_10_merchant_city_chart functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_10_merchant_state_chart(data_df: DataFrame, source: str) -> Chart:\n",
    "    \"\"\" Chart the proportion of the top 10s Merchant State in transactions. \"\"\"\n",
    "    data_df = data_df.top_k(10, by=\"Count\")\n",
    "    return alt.Chart(\n",
    "        data_df,\n",
    "        title=f\"Top 10 {source} Merchant State\").mark_arc(innerRadius=70).encode(\n",
    "        color=alt.Color(\"Merchant State\", title=\"Merchant State\", type=\"nominal\",\n",
    "                    sort='ascending',\n",
    "                    scale=alt.Scale(scheme='plasma')),\n",
    "        theta=\"Count\"\n",
    "    ).properties(width=400)\n",
    "\n",
    "def top_10_merchant_city_chart(data_df: DataFrame, source: str) -> Chart:\n",
    "    \"\"\" Chart the proportion of the top 10 Merchant Cities in transactions. \"\"\"\n",
    "    data_df = data_df.top_k(10, by=\"Count\")\n",
    "    return alt.Chart(\n",
    "        data_df,\n",
    "        title=f\"Top 10 {source} Merchant City\").mark_arc(innerRadius=70).encode(\n",
    "        color=alt.Color(\"Merchant City\", title=\"Merchant State\", type=\"nominal\",\n",
    "                    sort='ascending',\n",
    "                    scale=alt.Scale(scheme='plasma')),\n",
    "        theta=\"Count\"\n",
    "    ).properties(width=400)\n",
    "\n",
    "legit_state = new_data_df.select(\n",
    "    pl.col(\"Merchant State\"), pl.col(\"Is Fraud?\")).filter(pl.col(\"Is Fraud?\") == 0\n",
    "    ).group_by(\"Merchant State\").agg(\n",
    "        pl.col(\"Is Fraud?\").count().alias(\"Count\")\n",
    "    )\n",
    "legit_city = new_data_df.select(\n",
    "    pl.col(\"Merchant City\"), pl.col(\"Is Fraud?\")).filter(pl.col(\"Is Fraud?\") == 0\n",
    "    ).group_by(\"Merchant City\").agg(\n",
    "        pl.col(\"Is Fraud?\").count().alias(\"Count\")\n",
    "    )\n",
    "fraud_state = new_data_df.select(\n",
    "    pl.col(\"Merchant State\"), pl.col(\"Is Fraud?\")).filter(pl.col(\"Is Fraud?\") == 1\n",
    "    ).group_by(\"Merchant State\").agg(\n",
    "        pl.col(\"Is Fraud?\").count().alias(\"Count\")\n",
    "    )\n",
    "fraud_city = new_data_df.select(\n",
    "    pl.col(\"Merchant City\"), pl.col(\"Is Fraud?\")).filter(pl.col(\"Is Fraud?\") == 1\n",
    "    ).group_by(\"Merchant City\").agg(\n",
    "        pl.col(\"Is Fraud?\").count().alias(\"Count\")\n",
    "    )\n",
    "print(legit_state.sort(\"Count\", descending=True))\n",
    "top_10_merchant_state_chart(legit_state, \"Legitimate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(legit_city.sort(\"Count\", descending=True))\n",
    "top_10_merchant_city_chart(legit_city, \"Legitimate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fraud_state.sort(\"Count\", descending=True))\n",
    "top_10_merchant_state_chart(fraud_state, \"Fraudulent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fraud_city.sort(\"Count\", descending=True))\n",
    "top_10_merchant_city_chart(fraud_city, \"Fraudulent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the previous charts we can see that online transactions are dominant for the merchant state and city in both leigitimate and fraudulent transactions. However we can also see that for fraudulent transactions, the states and cities include locations outside of the US, while for legitimate transactions they are mostly in the US except for the online transactions. This can be mostly explained because this dataset was generated to simulate US based people, so we can expect most of their legitimate transactions to come from US locations.\n",
    "\n",
    "Now let's see if the Errors? column varies significantly between legitimate and fraudulent transactions, using an errors_chart function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def errors_chart(data_df: DataFrame, source: str) -> Chart:\n",
    "    \"\"\" Chart the proportion of Errors? in transactions. \"\"\"\n",
    "    return alt.Chart(\n",
    "        data_df,\n",
    "        title=f\"{source} Errors?\").mark_arc(innerRadius=70).encode(\n",
    "        color=alt.Color(\"Errors?\", title=\"Errors?\", type=\"nominal\",\n",
    "                    sort='ascending',\n",
    "                    scale=alt.Scale(scheme='plasma')),\n",
    "        theta=\"Count\"\n",
    "    ).properties(width=400)\n",
    "\n",
    "legit_errors = new_data_df.select(\n",
    "    pl.col(\"Errors?\"), pl.col(\"Is Fraud?\")).filter(\n",
    "        pl.col(\"Is Fraud?\") == 0).filter(pl.col(\"Errors?\") != \"No\"\n",
    "    ).group_by(\"Errors?\").agg(\n",
    "        pl.col(\"Is Fraud?\").count().alias(\"Count\")\n",
    "    )\n",
    "fraud_errors = new_data_df.select(\n",
    "    pl.col(\"Errors?\"), pl.col(\"Is Fraud?\")).filter(\n",
    "        pl.col(\"Is Fraud?\") == 1).filter(pl.col(\"Errors?\") != \"No\"\n",
    "    ).group_by(\"Errors?\").agg(\n",
    "        pl.col(\"Is Fraud?\").count().alias(\"Count\")\n",
    "    )\n",
    "print(legit_errors.sort(\"Count\", descending=True))\n",
    "errors_chart(legit_errors, \"Legitimate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fraud_errors.sort(\"Count\", descending=True))\n",
    "errors_chart(fraud_errors, \"Fraudulent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the previous charts we can see that Insufficient Balance is the most common error for both legitimate and fraudulent transactions.\n",
    "\n",
    "Now let's see how the Merchant Category Codes (MCC) vary between fraudulent and legitimate transactions using a top_10_mcc_chart function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_10_mcc_chart(data_df: DataFrame, source: str) -> Chart:\n",
    "    \"\"\" Chart the proportion of Merchant Category Codes in transactions. \"\"\"\n",
    "    data_df = data_df.top_k(10, by=\"Count\")\n",
    "    return alt.Chart(\n",
    "        data_df,\n",
    "        title=f\"Top 10 {source} MCC\").mark_arc(innerRadius=70).encode(\n",
    "        color=alt.Color(\"MCC\", title=\"MCC\", type=\"nominal\",\n",
    "                    sort='ascending',\n",
    "                    scale=alt.Scale(scheme='plasma')),\n",
    "        theta=\"Count\"\n",
    "    ).properties(width=400)\n",
    "\n",
    "legit_mcc = new_data_df.select(\n",
    "    pl.col(\"MCC\"), pl.col(\"Is Fraud?\")).filter(pl.col(\"Is Fraud?\") == 0\n",
    "    ).group_by(\"MCC\").agg(\n",
    "        pl.col(\"Is Fraud?\").count().alias(\"Count\")\n",
    "    )\n",
    "fraud_mcc = new_data_df.select(\n",
    "    pl.col(\"MCC\"), pl.col(\"Is Fraud?\")).filter(pl.col(\"Is Fraud?\") == 1\n",
    "    ).group_by(\"MCC\").agg(\n",
    "        pl.col(\"Is Fraud?\").count().alias(\"Count\")\n",
    "    )\n",
    "\n",
    "print(legit_mcc.sort(\"Count\", descending=True))\n",
    "top_10_mcc_chart(legit_mcc, \"Legitimate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fraud_mcc.sort(\"Count\", descending=True))\n",
    "top_10_mcc_chart(fraud_mcc, \"Fraudulent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the most used Merchant Category Codes vary between fraudulent and legitimate transactions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Creation and Evaluation\n",
    "\n",
    "### Baseline Model\n",
    "\n",
    "Before creating our final model, we will develop a very simple model to decide if a transaction is legitimate or not, solely based on the merchant state, city, zip, MCC and Error values in transactions that were previously marked as fraudulent, and then calculate the model's recall. The final model should improve this baseline model's recall by at least 70%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_accuracy': 0.7799038281481725,\n",
       " 'train_recall': 0.005539741935722937,\n",
       " 'validate_accuracy': 0.7795626613742077,\n",
       " 'validate_recall': 0.0050274208929613024,\n",
       " 'test_accuracy': 0.7799762811767208,\n",
       " 'test_recall': 0.005025468557859475}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score  # noqa: E402\n",
    "from sklearn.model_selection import train_test_split  # noqa: E402\n",
    "\n",
    "\n",
    "def predict_evaluate(fraud_merchant_states: list, fraud_merchant_cities: list,\n",
    "            fraud_zips: list, fraud_mccs: list, fraud_errors: list,\n",
    "            x_values: DataFrame, y_values: DataFrame) -> (float, float):\n",
    "    \"\"\" Take the learned fraudulent values and predict based on them, calculate\n",
    "    and return metrics.\n",
    "    \"\"\"\n",
    "\n",
    "    pred_train_y = x_values.select(\n",
    "        pred_train_y = (\n",
    "            pl.col(\"Merchant State\").is_in(fraud_merchant_states) &\n",
    "            pl.col(\"Merchant City\").is_in(fraud_merchant_cities) &\n",
    "            pl.col(\"Zip\").is_in(fraud_zips) &\n",
    "            pl.col(\"MCC\").is_in(fraud_mccs) &\n",
    "            pl.col(\"Errors?\").is_in(fraud_errors)\n",
    "        ).map_elements(lambda x: 1 if x is True else 0)\n",
    "    )\n",
    "    train_accuracy = accuracy_score(pred_train_y, y_values)\n",
    "    train_recall = recall_score(pred_train_y, y_values)\n",
    "    return train_accuracy, train_recall\n",
    "\n",
    "\n",
    "def baseline_model(data_df: DataFrame) -> dict:\n",
    "    \"\"\" This model learns the values used in fraudulent transactions and if the\n",
    "    values in the provided transactions are in the learned ones the transaction\n",
    "    is deemed as fraudulent, otherwise it is deemed legitimate.\n",
    "\n",
    "    Args:\n",
    "        data_df (DataFrame): A polars DataFrame with the transaction database.\n",
    "\n",
    "    Returns:\n",
    "        metrics (dict): The baseline model's metrics.\n",
    "    \"\"\"\n",
    "\n",
    "    is_fraud = data_df.select(pl.col('Is Fraud?'))\n",
    "    features = data_df.drop('Is Fraud?')\n",
    "\n",
    "    original_count = len(data_df)\n",
    "    training_size = int(original_count * .6)\n",
    "    test_size = int((1 - .6) * .5 * training_size)\n",
    "\n",
    "    train_x, rest_x, train_y, rest_y = train_test_split(features,\n",
    "                                                        is_fraud,\n",
    "                                                        train_size=training_size,\n",
    "                                                        random_state=0)\n",
    "    validate_x, test_x, validate_y, test_y  = train_test_split(rest_x,\n",
    "                                                               rest_y,\n",
    "                                                               train_size=test_size,\n",
    "                                                               random_state=0)\n",
    "    # Learn fraudulent values\n",
    "    train_x = train_x.with_row_index()\n",
    "    fraud_index = train_y.with_row_index().filter(\n",
    "        pl.col(\"Is Fraud?\") == 1).select(\n",
    "            pl.col('index')).to_series().to_list()\n",
    "    fraud_merchant_states = train_x.filter(\n",
    "        pl.col(\"index\").is_in(fraud_index)\n",
    "    ).select(\n",
    "        pl.col(\"Merchant State\").unique()\n",
    "    ).to_series().to_list()\n",
    "    fraud_merchant_cities = train_x.filter(\n",
    "        pl.col(\"index\").is_in(fraud_index)\n",
    "    ).select(\n",
    "        pl.col(\"Merchant City\").unique()\n",
    "    ).to_series().to_list()\n",
    "    fraud_zips = train_x.filter(\n",
    "        pl.col(\"index\").is_in(fraud_index)\n",
    "    ).select(\n",
    "        pl.col(\"Zip\").unique()\n",
    "    ).to_series().to_list()\n",
    "    fraud_mccs = train_x.filter(\n",
    "        pl.col(\"index\").is_in(fraud_index)\n",
    "    ).select(\n",
    "        pl.col(\"MCC\").unique()\n",
    "    ).to_series().to_list()\n",
    "    fraud_errors = train_x.filter(\n",
    "        pl.col(\"index\").is_in(fraud_index)\n",
    "    ).select(\n",
    "        pl.col(\"Errors?\").unique()\n",
    "    ).to_series().to_list()\n",
    "\n",
    "    # Predict and evaluate vs train, validate and test data\n",
    "    train_accuracy, train_recall = predict_evaluate(\n",
    "        fraud_merchant_states, fraud_merchant_cities, fraud_zips, fraud_mccs,\n",
    "        fraud_errors, train_x, train_y\n",
    "    )\n",
    "    validate_accuracy, validate_recall = predict_evaluate(\n",
    "        fraud_merchant_states, fraud_merchant_cities, fraud_zips, fraud_mccs,\n",
    "        fraud_errors, validate_x, validate_y\n",
    "    )\n",
    "    test_accuracy, test_recall = predict_evaluate(\n",
    "        fraud_merchant_states, fraud_merchant_cities, fraud_zips, fraud_mccs,\n",
    "        fraud_errors, test_x, test_y\n",
    "    )\n",
    "\n",
    "    metrics = {\n",
    "        'train_accuracy': train_accuracy,\n",
    "        'train_recall': train_recall,\n",
    "        'validate_accuracy': validate_accuracy,\n",
    "        'validate_recall': validate_recall,\n",
    "        'test_accuracy': test_accuracy,\n",
    "        'test_recall': test_recall,\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "baseline_model(new_data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, our baseline model's recall is very low, so we should expect our final model to improve the recall by at least 70%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting our Dataset\n",
    "\n",
    "To be able to create, train and deploy our final machine learning model, first we need to create a function that will split our dataset in training, validation and test samples.\n",
    "\n",
    "For that purpose we will create a split_dataset function that receives our dataset, and the percentages of data that we will use to train our model, and to validate and test our model's performance.\n",
    "\n",
    "The first thing this function does is convert the columns with categorical data that are not integer values into their categorical \"physical\" (integer) values.\n",
    "\n",
    "Then it separates our dependent variable in the \"Is Fraud?\" column from the rest of the columns.\n",
    "\n",
    "Finally, it uses the train_test_split function from sklearn.model_selection, and we will call it with a random_state=0 value so it returns the same data split every time we call it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split  # noqa: E402\n",
    "\n",
    "\n",
    "def split_dataset(data_df: DataFrame, train_proportion: float,\n",
    "                  test_proportion: float):\n",
    "    \"\"\" Convert categorical columns and split dataset. \"\"\"\n",
    "    data_df = data_df.with_columns(\n",
    "        pl.col(\"Use Chip\").cast(pl.Categorical).to_physical()\n",
    "    ).with_columns(\n",
    "        pl.col(\"Merchant Name\").cast(pl.String).cast(\n",
    "            pl.Categorical).to_physical()\n",
    "    ).with_columns(\n",
    "        pl.col(\"Merchant City\").cast(pl.Categorical).to_physical()\n",
    "    ).with_columns(\n",
    "        pl.col(\"Merchant State\").cast(pl.Categorical).to_physical()\n",
    "    ).with_columns(\n",
    "        pl.col(\"Zip\").cast(pl.Categorical).to_physical()\n",
    "    ).with_columns(\n",
    "        pl.col(\"Errors?\").cast(pl.Categorical).to_physical()\n",
    "    )\n",
    "    is_fraud = data_df.select(pl.col('Is Fraud?'))\n",
    "    features = data_df.drop('Is Fraud?')\n",
    "\n",
    "    original_count = len(data_df)\n",
    "    training_size = int(original_count * train_proportion)\n",
    "    test_size = int((1 - train_proportion) * test_proportion * training_size)\n",
    "\n",
    "    train_x, rest_x, train_y, rest_y = train_test_split(features,\n",
    "                                                        is_fraud,\n",
    "                                                        train_size=training_size,\n",
    "                                                        random_state=0)\n",
    "    validate_x, test_x, validate_y, test_y  = train_test_split(rest_x,\n",
    "                                                               rest_y,\n",
    "                                                               train_size=test_size,\n",
    "                                                               random_state=0)\n",
    "\n",
    "    return (train_x, train_y), (validate_x, validate_y), (test_x, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering and Training Pipeline\n",
    "\n",
    "Next, we will create our feature engineering and model training pipeline. For that we will create a build_pipeline function that will peform both the feature engineering and train our model.\n",
    "\n",
    "#### Feature Engineering\n",
    "\n",
    "- We will use an scikit-learn [RobustScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html#sklearn.preprocessing.RobustScaler) to scale the Amount column, this will help us reduce the effect of amount outliers (values that are too high or too low) in our model.\n",
    "- At the beginning we were using OneHotEncoder for the categorical columns, however, we noticed it was adding too many columns to our dataset because the categorical columns have too many categories, so after investigating alternatives we decided to use a scikit-learn [TargetEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.TargetEncoder.html#sklearn.preprocessing.TargetEncoder) instead, to encode the Card, Merchant Name, Merchant City, Merchant State, Zip, MCC, Errors?, Hour, Minute, and Use Chip colums without adding new ones. The use of TargetEncoder does have its [trade-offs](https://www.pythonprog.com/sklearn-preprocessing-targetencoder/), so we need to keep them in mind as we build and test our model.\n",
    "\n",
    "#### Modeling\n",
    "- For our model, we will use an scikit-learn [RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) with 10 estimators, 10 parallel jobs, and set it to run in verbose mode. We added 10 estimators only so the model doesn't take too long to train (going from the default of 100 to 10 saves us around 50 minutes in training with this dataset), 10 parallel jobs to speed up the training process, and we run it in verbose mode so we can follow the training and predicting process as it happens.\n",
    "- We will measure both the accuracy and recall of our model, however, the most important metric will be the recall, because for this case in particular we need to be able to find all the positive fraudulent transactions in our dataset, which means reducing the number of false negatives, so accuracy alone would not be enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer  # noqa: E402\n",
    "from sklearn.ensemble import RandomForestClassifier  # noqa: E402\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline  # noqa: E402\n",
    "from sklearn.preprocessing import RobustScaler, TargetEncoder  # noqa: E402\n",
    "\n",
    "\n",
    "def build_pipeline():\n",
    "    \"\"\" Build pipeline with feature encoders and model. \"\"\"\n",
    "    # Target encoder\n",
    "    internal_target_encoding = TargetEncoder(smooth=\"auto\")\n",
    "    columns_to_encode = [\n",
    "        \"Card\",\n",
    "        \"Merchant Name\",\n",
    "        \"Merchant State\",\n",
    "        \"Merchant City\",\n",
    "        \"Zip\",\n",
    "        \"MCC\",\n",
    "        \"Errors?\",\n",
    "        \"Hour\",\n",
    "        \"Minute\",\n",
    "        \"Use Chip\"\n",
    "    ]\n",
    "\n",
    "    target_encoding = ColumnTransformer([\n",
    "        (\n",
    "            'target_encode',\n",
    "            internal_target_encoding,\n",
    "            columns_to_encode\n",
    "        )\n",
    "    ])\n",
    "\n",
    "    # Scaler\n",
    "    internal_scaler = RobustScaler()\n",
    "    columns_to_scale = [\"Amount\"]\n",
    "\n",
    "    scaler = ColumnTransformer([\n",
    "        (\"scaler\", internal_scaler, columns_to_scale)\n",
    "    ])\n",
    "\n",
    "    # Full pipeline\n",
    "    feature_engineering_pipeline  = Pipeline([\n",
    "        (\n",
    "            \"features\",\n",
    "            FeatureUnion([\n",
    "                ('categories', target_encoding),\n",
    "                ('scaled', scaler)\n",
    "            ])\n",
    "        )\n",
    "    ])\n",
    "\n",
    "    # Machine learning model\n",
    "    model = RandomForestClassifier(n_estimators=10, verbose=1, n_jobs=10)\n",
    "\n",
    "\n",
    "    # Full pipeline\n",
    "    final_pipeline = Pipeline([\n",
    "        (\"feature_engineering\", feature_engineering_pipeline),\n",
    "        (\"model\", model)\n",
    "    ])\n",
    "\n",
    "    return final_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training and Validation\n",
    "\n",
    "- We will create a model_training_validation function that will use the pipeline created with the build_pipeline function, and with it we will call the model's fit function to train the model with the training split of the dataset. \n",
    "- After that, we will use the pipeline to predict if the transactions included in the training, validation and testing splits are fraudulent or legitimate.\n",
    "- With the predicted values, we will calculate the accuracy and recall obtained with each split and save all the metrics in a 'metrics' dictionary that we will return along with our trained pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score  # noqa: E402\n",
    "\n",
    "\n",
    "def model_training_validation(final_pipeline: Pipeline,\n",
    "                              train_x: DataFrame, train_y: DataFrame,\n",
    "                              validate_x: DataFrame, validate_y: DataFrame,\n",
    "                              test_x: DataFrame, test_y: DataFrame):\n",
    "    \"\"\" Train and use the model to predict values for the validation and test\n",
    "    splits.\n",
    "    \"\"\"\n",
    "    final_pipeline.fit(train_x, train_y.to_numpy().ravel())\n",
    "\n",
    "    train_pred_y = final_pipeline.predict(train_x)\n",
    "    validate_pred_y = final_pipeline.predict(validate_x)\n",
    "    test_pred_y = final_pipeline.predict(test_x)\n",
    "\n",
    "    train_accuracy = accuracy_score(train_pred_y, train_y.to_numpy().ravel())\n",
    "    train_recall = recall_score(train_pred_y, train_y.to_numpy().ravel())\n",
    "\n",
    "    validate_accuracy = accuracy_score(validate_pred_y, validate_y.to_numpy().ravel())\n",
    "    validate_recall = recall_score(validate_pred_y, validate_y.to_numpy().ravel())\n",
    "\n",
    "    test_accuracy = accuracy_score(test_pred_y, test_y.to_numpy().ravel())\n",
    "    test_recall = recall_score(test_pred_y, test_y.to_numpy().ravel())\n",
    "\n",
    "    metrics = {\n",
    "        'train_accuracy': train_accuracy,\n",
    "        'train_recall': train_recall,\n",
    "        'validate_accuracy': validate_accuracy,\n",
    "        'validate_recall': validate_recall,\n",
    "        'test_accuracy': test_accuracy,\n",
    "        'test_recall': test_recall,\n",
    "    }\n",
    "\n",
    "    return final_pipeline, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Training Run\n",
    "\n",
    "- Finally, we will build a full_training_run function that will call our split_dataset, build_pipeline and model_training_validation functions to train our model and obtain the validation and test results.\n",
    "- It will also use joblib's dump bethod to save our model under /model/inference_pipeline.joblib when the write_model parameter is True, otherwise the model won't be re-written."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  # noqa: E402\n",
    "\n",
    "from joblib import dump  # noqa: E402\n",
    "\n",
    "\n",
    "def full_training_run(write_model: bool):\n",
    "    \"\"\" Split dataset, build pipeline, train and validate model, and write\n",
    "    model if write_model is True.\n",
    "    \"\"\"\n",
    "    training_data, validate_data, test_data = split_dataset(new_data_df,\n",
    "                                                            train_proportion=0.6,\n",
    "                                                            test_proportion=0.5)\n",
    "\n",
    "    training_pipeline = build_pipeline()\n",
    "\n",
    "    training_pipeline, metrics = model_training_validation(\n",
    "        training_pipeline,\n",
    "        train_x=training_data[0],\n",
    "        train_y=training_data[1],\n",
    "        validate_x=validate_data[0],\n",
    "        validate_y=validate_data[1],\n",
    "        test_x=test_data[0],\n",
    "        test_y=test_data[1]\n",
    "    )\n",
    "\n",
    "    print(metrics)\n",
    "\n",
    "    if write_model:\n",
    "        if os.path.exists(\"../model/inference_pipeline.joblib\"):\n",
    "            os.remove(\"../model/inference_pipeline.joblib\")\n",
    "        dump(training_pipeline, \"../model/inference_pipeline.joblib\",\n",
    "             compress=9)\n",
    "\n",
    "    return training_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run our full_training_run function with write_model set to True once, so it saves our final model to disk. After that, all subsequents runs should be run using write_model set to False so the saved model doesn't get overwritten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_training_run(write_model=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our final model's recall is very good compared to our baseline model, however, we can also see that it seems to suffer a bit from overfitting because the recall with the validation and testing splits is worse than that with the training one.\n",
    "\n",
    "## Error Analysis\n",
    "\n",
    "Let's take a look at the cases where the model misclassified legitimate transactions as fraudulent (false positives) and fraudulent transactions as legitimate (false negatives) and see if we can find a trend that could help us improve the model.\n",
    "\n",
    "For that we will modify our model_training_validation function a bit so it returns the predicted values and the values it should have predicted for each split of our dataset, so we can compare them and find the errors in the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_training_validation(final_pipeline: Pipeline,\n",
    "                              train_x: DataFrame, train_y: DataFrame,\n",
    "                              validate_x: DataFrame, test_x: DataFrame):\n",
    "\n",
    "    final_pipeline.fit(train_x, train_y.to_numpy().ravel())\n",
    "\n",
    "    train_pred_y = final_pipeline.predict(train_x)\n",
    "    validate_pred_y = final_pipeline.predict(validate_x)\n",
    "    test_pred_y = final_pipeline.predict(test_x)\n",
    "\n",
    "    return train_pred_y, validate_pred_y, test_pred_y\n",
    "\n",
    "training_data, validate_data, test_data = split_dataset(new_data_df,\n",
    "                                                            train_proportion=0.6,\n",
    "                                                            test_proportion=0.5)\n",
    "\n",
    "training_pipeline = build_pipeline()\n",
    "\n",
    "train_pred_y, validate_pred_y, test_pred_y = model_training_validation(\n",
    "    training_pipeline,\n",
    "    train_x=training_data[0],\n",
    "    train_y=training_data[1],\n",
    "    validate_x=validate_data[0],\n",
    "    test_x=test_data[0],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the predicted values for the validation and testing splits, let's compare them with the real values and find the false positives and false negatives of each split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_x = validate_data[0].with_row_index()\n",
    "validate_y = validate_data[1]\n",
    "test_x = test_data[0].with_row_index()\n",
    "test_y = test_data[1]\n",
    "\n",
    "# transform predicted values into DataFrames\n",
    "validate_pred_y = pl.DataFrame({\"predicted_Is_Fraud?\": validate_pred_y})\n",
    "test_pred_y = pl.DataFrame({\"predicted_Is_Fraud?\": test_pred_y})\n",
    "\n",
    "# get new DataFrame with predicted vs real values\n",
    "compare_validate_y = pl.concat([validate_y, validate_pred_y], how=\"horizontal\")\n",
    "compare_test_y = pl.concat([test_y, test_pred_y], how=\"horizontal\")\n",
    "\n",
    "# errors = where the predicted value is different than real one\n",
    "errors_validate_y = compare_validate_y.select(\n",
    "    pl.col(\"predicted_Is_Fraud?\"),\n",
    "    errors=(pl.col(\"Is Fraud?\") != pl.col(\"predicted_Is_Fraud?\"))\n",
    ").with_row_index().filter(\n",
    "    pl.col(\"errors\")\n",
    ")\n",
    "errors_test_y = compare_test_y.select(\n",
    "    pl.col(\"predicted_Is_Fraud?\"),\n",
    "    errors=(pl.col(\"Is Fraud?\") != pl.col(\"predicted_Is_Fraud?\"))\n",
    ").with_row_index().filter(\n",
    "    pl.col(\"errors\")\n",
    ")\n",
    "\n",
    "# false positives = where predicted_Is_Fraud is 1\n",
    "false_pos_idx_validate_y = errors_validate_y.filter(\n",
    "    pl.col(\"index\") & pl.col(\"predicted_Is_Fraud?\") == 1).to_series().to_list()\n",
    "false_pos_idx_test_y = errors_test_y.filter(\n",
    "    pl.col(\"index\") & pl.col(\"predicted_Is_Fraud?\") == 1).to_series().to_list()\n",
    "\n",
    "# false positives = where predicted_Is_Fraud is 0\n",
    "false_neg_idx_validate_y = errors_validate_y.filter(\n",
    "    pl.col(\"index\") & pl.col(\"predicted_Is_Fraud?\") == 0).to_series().to_list()\n",
    "false_neg_idx_test_y = errors_test_y.filter(\n",
    "    pl.col(\"index\") & pl.col(\"predicted_Is_Fraud?\") == 0).to_series().to_list()\n",
    "\n",
    "\n",
    "# get features corresponding with the error idx's\n",
    "false_pos_validate_x = validate_x.filter(\n",
    "    pl.col(\"index\").is_in(false_pos_idx_validate_y))\n",
    "false_neg_validate_x = validate_x.filter(\n",
    "    pl.col(\"index\").is_in(false_neg_idx_validate_y))\n",
    "\n",
    "# get features corresponding with the error idx's\n",
    "false_pos_test_x = test_x.filter(pl.col(\"index\").is_in(false_pos_idx_test_y))\n",
    "false_neg_test_x = test_x.filter(pl.col(\"index\").is_in(false_neg_idx_test_y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's display each dataset and see if we can find any patterns or obvious things to fix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_neg_validate_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_pos_validate_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_neg_test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_pos_test_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While there doesn't seem to be an obvious pattern in the features for the false positives and false negatives, there's a disproportionately higher number of false negatives vs false positives. This could be caused by a disproportion in the number of positive cases (fraudulent transactions) vs negative cases (legitimate transactions) in our dataset. To see if that's the case, let's evaluate how many fraudulent trasactions we have in our dataset in comparison with the legitimate ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fraud_vs_legitimate_chart(data_df: DataFrame) -> Chart:\n",
    "    \"\"\" Bar chart with the legitimate vs fraudulent transaction numbers. \"\"\"\n",
    "    top_amount = data_df.top_k(1, by=\"count\").to_numpy()[0][1]\n",
    "    print(top_amount)\n",
    "    return alt.Chart(\n",
    "        data_df,\n",
    "        title=\"Fraudulent vs Legitimate Transactions\").mark_bar().encode(\n",
    "        x=alt.X('Is Fraud?:O', axis=alt.Axis(labelAngle=-45)),\n",
    "        y=\"count\",\n",
    "        color=alt.condition(\n",
    "            alt.datum.count == top_amount,\n",
    "            alt.value('orange'),\n",
    "            alt.value('steelblue'))\n",
    "    ).properties(width=400)\n",
    "\n",
    "fraud_vs_legit = new_data_df.select(\n",
    "    pl.col('Is Fraud?')).to_series().value_counts().cast(\n",
    "    {\"count\": pl.Int64})\n",
    "print(new_data_df.select(\n",
    "    pl.col('Is Fraud?')).to_series().value_counts())\n",
    "fraud_vs_legitimate_chart(fraud_vs_legit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Analysis Results and Conclusions\n",
    "\n",
    "- As we can see, the fraudulent transactions are way underrepresented in comparison with the legitimate ones.\n",
    "- As it was mentioned in the article we included earlier about [TargetEncoder](https://www.pythonprog.com/sklearn-preprocessing-targetencoder/), one of its limitations was that it could be sensitive to imbalanced classes like the imbalance we are seeing precisely here, so it is probably the reason behind the reduced recall we are seeing, so we could try to fix that in our model_training_validation function and see if that improves the recall of our final model.\n",
    "- For that we will use [Synthetic Minority Over-sampling TEchnique (SMOTE)](https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTE.html) to resample the fraudulent cases in our dataset so they are more balanced with the legitimate ones.\n",
    "- SMOTE is a very powerful tool that is used to [address class imbalance in a dataset](https://medium.com/@corymaklin/synthetic-minority-over-sampling-technique-smote-7d419696b88c)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE  # noqa: E402\n",
    "from sklearn.metrics import accuracy_score, recall_score  # noqa: E402, F811\n",
    "\n",
    "\n",
    "def model_training_validation(final_pipeline: Pipeline,\n",
    "                              train_x: DataFrame, train_y: DataFrame,\n",
    "                              validate_x: DataFrame, validate_y: DataFrame,\n",
    "                              test_x: DataFrame, test_y: DataFrame):\n",
    "    \"\"\" Before training and testing our model we will try to fix the dataset's\n",
    "    fraud/legit case imbalance using SMOTE's fit_resample method.\n",
    "    \"\"\"\n",
    "    sm = SMOTE(random_state=0)\n",
    "\n",
    "    train_x_res, train_y_res = sm.fit_resample(\n",
    "        train_x.to_pandas(), train_y.to_pandas())\n",
    "    validate_x_res, validate_y_res = sm.fit_resample(\n",
    "        validate_x.to_pandas(), validate_y.to_pandas())\n",
    "    test_x_res, test_y_res = sm.fit_resample(\n",
    "        test_x.to_pandas(), test_y.to_pandas())\n",
    "\n",
    "    final_pipeline.fit(train_x_res, train_y_res.to_numpy().ravel())\n",
    "\n",
    "    train_pred_y = final_pipeline.predict(train_x_res)\n",
    "    validate_pred_y = final_pipeline.predict(validate_x_res)\n",
    "    test_pred_y = final_pipeline.predict(test_x_res)\n",
    "\n",
    "    train_accuracy = accuracy_score(train_pred_y, train_y_res.to_numpy().ravel())\n",
    "    train_recall = recall_score(train_pred_y, train_y_res.to_numpy().ravel())\n",
    "\n",
    "    validate_accuracy = accuracy_score(validate_pred_y, validate_y_res.to_numpy().ravel())\n",
    "    validate_recall = recall_score(validate_pred_y, validate_y_res.to_numpy().ravel())\n",
    "\n",
    "    test_accuracy = accuracy_score(test_pred_y, test_y_res.to_numpy().ravel())\n",
    "    test_recall = recall_score(test_pred_y, test_y_res.to_numpy().ravel())\n",
    "\n",
    "    metrics = {\n",
    "        'train_accuracy': train_accuracy,\n",
    "        'train_recall': train_recall,\n",
    "        'validate_accuracy': validate_accuracy,\n",
    "        'validate_recall': validate_recall,\n",
    "        'test_accuracy': test_accuracy,\n",
    "        'test_recall': test_recall,\n",
    "    }\n",
    "\n",
    "    return final_pipeline, metrics\n",
    "\n",
    "full_training_run(write_model=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, after using SMOTE we get better recall values with our training, validation and test splits.\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In order to be able to use all the functions in this notebook, we will save them all in separate python files (fraud_detection_flow.py and feature_pipeline.py) to achieve the following:\n",
    "- We will build a training pipeline using Metaflow.\n",
    "- We will use mlflow to build and keep model registry.\n",
    "- We will use BentoML to create a contained model in Docker ready to be used in Production environments."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
